{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 - Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.misc\n",
    "from tensorflow.keras.applications.resnet_v2 import ResNet50V2\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.resnet_v2 import preprocess_input, decode_predictions\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from resnets_utils import *\n",
    "from tensorflow.keras.initializers import random_uniform, glorot_uniform, constant, identity\n",
    "from tensorflow.python.framework.ops import EagerTensor\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "from test_utils import summary, comparator\n",
    "import public_tests\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Here is where you're actually using the power of the Functional API to create a shortcut path: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-df47af4847e5335f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# UNQ_C1\n",
    "# GRADED FUNCTION: convolutional_block\n",
    "\n",
    "def identity_block(X, f, filters, training=True, initializer=random_uniform):\n",
    "    # The building block model that has skip conections\n",
    "    \n",
    "    # Retrieve Filters\n",
    "    F1, F2, F3 = filters\n",
    "    \n",
    "    # Save the input value. You'll need this later to add back to the main path. \n",
    "    X_shortcut = X\n",
    "    \n",
    "    # First component of main path\n",
    "    X = Conv2D(filters = F1, kernel_size = 1, strides = (1,1), padding = 'valid', kernel_initializer = initializer(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3)(X, training = training) # Default axis\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    ### START CODE HERE\n",
    "    ## Second component of main path (≈3 lines)\n",
    "    X = Conv2D(filters = F2, kernel_size = f, strides = (1,1), padding = 'same', kernel_initializer = initializer(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3)(X,training=training)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    ## Third component of main path (≈2 lines)\n",
    "    X = Conv2D(filters = F3, kernel_size = 1, strides = (1,1), padding = 'valid', kernel_initializer = initializer(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3)(X, training = training)\n",
    "    \n",
    "    ## Add the original value we started at and the output value of the second layer.\n",
    "    X = Add()([X, X_shortcut]) #no np.add()\n",
    "    X = Activation('relu')(X) \n",
    "    ### END CODE HERE\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-e73a8466b807e261",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mWith training=False\u001b[0m\n",
      "\n",
      "[[[  0.        0.        0.        0.     ]\n",
      "  [  0.        0.        0.        0.     ]]\n",
      "\n",
      " [[192.71234 192.71234 192.71234  96.85617]\n",
      "  [ 96.85617  96.85617  96.85617  48.92808]]\n",
      "\n",
      " [[578.1371  578.1371  578.1371  290.5685 ]\n",
      "  [290.5685  290.5685  290.5685  146.78426]]]\n",
      "96.85617\n",
      "\n",
      "\u001b[1mWith training=True\u001b[0m\n",
      "\n",
      "[[[0.      0.      0.      0.     ]\n",
      "  [0.      0.      0.      0.     ]]\n",
      "\n",
      " [[0.40739 0.40739 0.40739 0.40739]\n",
      "  [0.40739 0.40739 0.40739 0.40739]]\n",
      "\n",
      " [[4.99991 4.99991 4.99991 3.25948]\n",
      "  [3.25948 3.25948 3.25948 2.40739]]]\n",
      "\u001b[32mAll tests passed!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "X1 = np.ones((1, 4, 4, 3)) * -1\n",
    "X2 = np.ones((1, 4, 4, 3)) * 1\n",
    "X3 = np.ones((1, 4, 4, 3)) * 3\n",
    "\n",
    "X = np.concatenate((X1, X2, X3), axis = 0).astype(np.float32)\n",
    "\n",
    "A3 = identity_block(X, f=2, filters=[4, 4, 3],\n",
    "                   initializer=lambda seed=0:constant(value=1),\n",
    "                   training=False)\n",
    "print('\\033[1mWith training=False\\033[0m\\n')\n",
    "A3np = A3.numpy()\n",
    "print(np.around(A3.numpy()[:,(0,-1),:,:].mean(axis = 3), 5))\n",
    "resume = A3np[:,(0,-1),:,:].mean(axis = 3)\n",
    "print(resume[1, 1, 0])\n",
    "\n",
    "print('\\n\\033[1mWith training=True\\033[0m\\n')\n",
    "np.random.seed(1)\n",
    "A4 = identity_block(X, f=2, filters=[3, 3, 3],\n",
    "                   initializer=lambda seed=0:constant(value=1),\n",
    "                   training=True)\n",
    "print(np.around(A4.numpy()[:,(0,-1),:,:].mean(axis = 3), 5))\n",
    "\n",
    "public_tests.identity_block_test(identity_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0017b68317ffa974",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# UNQ_C2\n",
    "# GRADED FUNCTION: convolutional_block\n",
    "# Used in case dimensions do not add up for a skip connection\n",
    "def convolutional_block(X, f, filters, s = 2, training=True, initializer=glorot_uniform):\n",
    "    \n",
    "    F1, F2, F3 = filters\n",
    "    \n",
    "    # Save the input value\n",
    "    # A dummy variable to store the original value (a(l))\n",
    "    X_shortcut = X\n",
    "    \n",
    "    #First two layers borrow from the identity_block\n",
    "\n",
    "\n",
    "    \n",
    "    # Start with your first convolution and use valid strides which allows for sizes to decrease\n",
    "    X = Conv2D(filters = F1, kernel_size = 1, strides = (s, s), padding='valid', kernel_initializer = initializer(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3)(X, training=training)\n",
    "    X = Activation('relu')(X) \n",
    "\n",
    "    \n",
    "    ## The second block does not have images sizes changing. since padding is set to same.\n",
    "    X = Conv2D(filters = F2, kernel_size = f, strides = (1,1), padding = 'same', kernel_initializer = initializer(seed=0))(X)\n",
    "    X = BatchNormalization(axis=3)(X,training=training)\n",
    "    X = Activation('relu')(X)\n",
    "\n",
    "    ## Third component of main path (≈2 lines)\n",
    "    X = Conv2D(filters = F3, kernel_size = 1, strides = (1, 1), padding='valid', kernel_initializer = initializer(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3)(X, training=training) \n",
    "    \n",
    "    #Finish three sets \n",
    "    \n",
    "    # Access the original image/activation. Then feed it through the last layer. Sizes get maintained.\n",
    "    X_shortcut = Conv2D(filters = F3, kernel_size = 1, strides = (s, s), padding='valid', kernel_initializer = initializer(seed=0))(X_shortcut)\n",
    "    X_shortcut = BatchNormalization(axis = 3)(X_shortcut, training=training)\n",
    "    \n",
    "    #Now add and activate\n",
    "    X = Add()([X, X_shortcut]) #np.add will fail\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-95c291eb244218fe",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[0.         0.66683817 0.         0.         0.88853896 0.5274254 ]\n",
      "  [0.         0.65053666 0.         0.         0.89592844 0.49965227]]\n",
      "\n",
      " [[0.         0.6312079  0.         0.         0.8636247  0.47643146]\n",
      "  [0.         0.5688321  0.         0.         0.85534114 0.41709304]]], shape=(2, 2, 6), dtype=float32)\n",
      "\u001b[92mAll tests passed!\n"
     ]
    }
   ],
   "source": [
    "from outputs import convolutional_block_output1, convolutional_block_output2\n",
    "np.random.seed(1)\n",
    "#X = np.random.randn(3, 4, 4, 6).astype(np.float32)\n",
    "X1 = np.ones((1, 4, 4, 3)) * -1\n",
    "X2 = np.ones((1, 4, 4, 3)) * 1\n",
    "X3 = np.ones((1, 4, 4, 3)) * 3\n",
    "\n",
    "X = np.concatenate((X1, X2, X3), axis = 0).astype(np.float32)\n",
    "\n",
    "A = convolutional_block(X, f = 2, filters = [2, 4, 6], training=False)\n",
    "\n",
    "assert type(A) == EagerTensor, \"Use only tensorflow and keras functions\"\n",
    "assert tuple(tf.shape(A).numpy()) == (3, 2, 2, 6), \"Wrong shape.\"\n",
    "assert np.allclose(A.numpy(), convolutional_block_output1), \"Wrong values when training=False.\"\n",
    "print(A[0])\n",
    "\n",
    "B = convolutional_block(X, f = 2, filters = [2, 4, 6], training=True)\n",
    "assert np.allclose(B.numpy(), convolutional_block_output2), \"Wrong values when training=True.\"\n",
    "\n",
    "print('\\033[92mAll tests passed!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected value**\n",
    "\n",
    "```\n",
    "tf.Tensor(\n",
    "[[[0.         0.66683817 0.         0.         0.88853896 0.5274254 ]\n",
    "  [0.         0.65053666 0.         0.         0.89592844 0.49965227]]\n",
    "\n",
    " [[0.         0.6312079  0.         0.         0.8636247  0.47643146]\n",
    "  [0.         0.5688321  0.         0.         0.85534114 0.41709304]]], shape=(2, 2, 6), dtype=float32)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-10dc95a4cf6275b9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# UNQ_C3\n",
    "# GRADED FUNCTION: ResNet50\n",
    "\n",
    "def ResNet50(input_shape = (64, 64, 3), classes = 6):\n",
    "    #our own custom ResNet\n",
    "    \n",
    "    # Define the input as a tensor with shape input_shape\n",
    "    X_input = Input(input_shape)\n",
    "\n",
    "    \n",
    "    # Zero-Padding\n",
    "    X = ZeroPadding2D((3, 3))(X_input)\n",
    "    \n",
    "    # Stage 1\n",
    "    X = Conv2D(64, (7, 7), strides = (2, 2), kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    X = BatchNormalization(axis = 3)(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n",
    "\n",
    "    # Stage 2\n",
    "    X = convolutional_block(X, f = 3, filters = [64, 64, 256], s = 1)\n",
    "    X = identity_block(X, 3, [64, 64, 256])\n",
    "    X = identity_block(X, 3, [64, 64, 256])\n",
    "\n",
    "    ### START CODE HERE\n",
    "    \n",
    "    ## Stage 3 (≈4 lines)\n",
    "    X = convolutional_block(X, f = 3, filters = [128, 128, 512], s = 2) \n",
    "    X = identity_block(X, 3, [128, 128, 512]) \n",
    "    X = identity_block(X, 3, [128, 128, 512]) \n",
    "    X = identity_block(X, 3, [128, 128, 512])  \n",
    "    \n",
    "    ## Stage 4 (≈6 lines)\n",
    "    X = convolutional_block(X, f = 3, filters = [256, 256, 1024], s = 2) \n",
    "    X = identity_block(X, 3, [256, 256, 1024])\n",
    "    X = identity_block(X, 3, [256, 256, 1024])\n",
    "    X = identity_block(X, 3, [256, 256, 1024])\n",
    "    X = identity_block(X, 3, [256, 256, 1024])\n",
    "    X = identity_block(X, 3, [256, 256, 1024])  \n",
    "\n",
    "    ## Stage 5 (≈3 lines)\n",
    "    X = convolutional_block(X, f = 3, filters = [512, 512, 2048], s = 2) \n",
    "    X = identity_block(X, 3, [512, 512, 2048])\n",
    "    X = identity_block(X, 3, [512, 512, 2048]) \n",
    "\n",
    "    ## AVGPOOL (≈1 line). Use \"X = AveragePooling2D(...)(X)\"\n",
    "    X = AveragePooling2D(pool_size=(2, 2))(X) \n",
    "    \n",
    "    ### END CODE HERE\n",
    "\n",
    "    # output layer\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(classes, activation='softmax', kernel_initializer = glorot_uniform(seed=0))(X)\n",
    "    \n",
    "    \n",
    "    # Create model\n",
    "    model = Model(inputs = X_input, outputs = X)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following code to build the model's graph. If your implementation is incorrect, you'll know it by checking your accuracy when running `model.fit(...)` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            [(None, 64, 64, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_8 (ZeroPadding2D (None, 70, 70, 3)    0           input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_196 (Conv2D)             (None, 32, 32, 64)   9472        zero_padding2d_8[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_196 (BatchN (None, 32, 32, 64)   256         conv2d_196[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_175 (Activation)     (None, 32, 32, 64)   0           batch_normalization_196[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2D)  (None, 15, 15, 64)   0           activation_175[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_197 (Conv2D)             (None, 15, 15, 64)   4160        max_pooling2d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_197 (BatchN (None, 15, 15, 64)   256         conv2d_197[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_176 (Activation)     (None, 15, 15, 64)   0           batch_normalization_197[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_198 (Conv2D)             (None, 15, 15, 64)   36928       activation_176[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_198 (BatchN (None, 15, 15, 64)   256         conv2d_198[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_177 (Activation)     (None, 15, 15, 64)   0           batch_normalization_198[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_199 (Conv2D)             (None, 15, 15, 256)  16640       activation_177[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_200 (Conv2D)             (None, 15, 15, 256)  16640       max_pooling2d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_199 (BatchN (None, 15, 15, 256)  1024        conv2d_199[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_200 (BatchN (None, 15, 15, 256)  1024        conv2d_200[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_47 (Add)                    (None, 15, 15, 256)  0           batch_normalization_199[0][0]    \n",
      "                                                                 batch_normalization_200[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_178 (Activation)     (None, 15, 15, 256)  0           add_47[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_201 (Conv2D)             (None, 15, 15, 64)   16448       activation_178[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_201 (BatchN (None, 15, 15, 64)   256         conv2d_201[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_179 (Activation)     (None, 15, 15, 64)   0           batch_normalization_201[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_202 (Conv2D)             (None, 15, 15, 64)   36928       activation_179[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_202 (BatchN (None, 15, 15, 64)   256         conv2d_202[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_180 (Activation)     (None, 15, 15, 64)   0           batch_normalization_202[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_203 (Conv2D)             (None, 15, 15, 256)  16640       activation_180[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_203 (BatchN (None, 15, 15, 256)  1024        conv2d_203[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_48 (Add)                    (None, 15, 15, 256)  0           batch_normalization_203[0][0]    \n",
      "                                                                 activation_178[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_181 (Activation)     (None, 15, 15, 256)  0           add_48[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_204 (Conv2D)             (None, 15, 15, 64)   16448       activation_181[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_204 (BatchN (None, 15, 15, 64)   256         conv2d_204[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_182 (Activation)     (None, 15, 15, 64)   0           batch_normalization_204[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_205 (Conv2D)             (None, 15, 15, 64)   36928       activation_182[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_205 (BatchN (None, 15, 15, 64)   256         conv2d_205[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_183 (Activation)     (None, 15, 15, 64)   0           batch_normalization_205[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_206 (Conv2D)             (None, 15, 15, 256)  16640       activation_183[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_206 (BatchN (None, 15, 15, 256)  1024        conv2d_206[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_49 (Add)                    (None, 15, 15, 256)  0           batch_normalization_206[0][0]    \n",
      "                                                                 activation_181[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_184 (Activation)     (None, 15, 15, 256)  0           add_49[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_207 (Conv2D)             (None, 8, 8, 128)    32896       activation_184[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_207 (BatchN (None, 8, 8, 128)    512         conv2d_207[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_185 (Activation)     (None, 8, 8, 128)    0           batch_normalization_207[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_208 (Conv2D)             (None, 8, 8, 128)    147584      activation_185[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_208 (BatchN (None, 8, 8, 128)    512         conv2d_208[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_186 (Activation)     (None, 8, 8, 128)    0           batch_normalization_208[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_209 (Conv2D)             (None, 8, 8, 512)    66048       activation_186[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_210 (Conv2D)             (None, 8, 8, 512)    131584      activation_184[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_209 (BatchN (None, 8, 8, 512)    2048        conv2d_209[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_210 (BatchN (None, 8, 8, 512)    2048        conv2d_210[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_50 (Add)                    (None, 8, 8, 512)    0           batch_normalization_209[0][0]    \n",
      "                                                                 batch_normalization_210[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_187 (Activation)     (None, 8, 8, 512)    0           add_50[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_211 (Conv2D)             (None, 8, 8, 128)    65664       activation_187[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_211 (BatchN (None, 8, 8, 128)    512         conv2d_211[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_188 (Activation)     (None, 8, 8, 128)    0           batch_normalization_211[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_212 (Conv2D)             (None, 8, 8, 128)    147584      activation_188[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_212 (BatchN (None, 8, 8, 128)    512         conv2d_212[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_189 (Activation)     (None, 8, 8, 128)    0           batch_normalization_212[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_213 (Conv2D)             (None, 8, 8, 512)    66048       activation_189[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_213 (BatchN (None, 8, 8, 512)    2048        conv2d_213[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_51 (Add)                    (None, 8, 8, 512)    0           batch_normalization_213[0][0]    \n",
      "                                                                 activation_187[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_190 (Activation)     (None, 8, 8, 512)    0           add_51[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_214 (Conv2D)             (None, 8, 8, 128)    65664       activation_190[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_214 (BatchN (None, 8, 8, 128)    512         conv2d_214[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_191 (Activation)     (None, 8, 8, 128)    0           batch_normalization_214[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_215 (Conv2D)             (None, 8, 8, 128)    147584      activation_191[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_215 (BatchN (None, 8, 8, 128)    512         conv2d_215[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_192 (Activation)     (None, 8, 8, 128)    0           batch_normalization_215[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_216 (Conv2D)             (None, 8, 8, 512)    66048       activation_192[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_216 (BatchN (None, 8, 8, 512)    2048        conv2d_216[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_52 (Add)                    (None, 8, 8, 512)    0           batch_normalization_216[0][0]    \n",
      "                                                                 activation_190[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_193 (Activation)     (None, 8, 8, 512)    0           add_52[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_217 (Conv2D)             (None, 8, 8, 128)    65664       activation_193[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_217 (BatchN (None, 8, 8, 128)    512         conv2d_217[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_194 (Activation)     (None, 8, 8, 128)    0           batch_normalization_217[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_218 (Conv2D)             (None, 8, 8, 128)    147584      activation_194[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_218 (BatchN (None, 8, 8, 128)    512         conv2d_218[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_195 (Activation)     (None, 8, 8, 128)    0           batch_normalization_218[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_219 (Conv2D)             (None, 8, 8, 512)    66048       activation_195[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_219 (BatchN (None, 8, 8, 512)    2048        conv2d_219[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_53 (Add)                    (None, 8, 8, 512)    0           batch_normalization_219[0][0]    \n",
      "                                                                 activation_193[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_196 (Activation)     (None, 8, 8, 512)    0           add_53[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_220 (Conv2D)             (None, 4, 4, 256)    131328      activation_196[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_220 (BatchN (None, 4, 4, 256)    1024        conv2d_220[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_197 (Activation)     (None, 4, 4, 256)    0           batch_normalization_220[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_221 (Conv2D)             (None, 4, 4, 256)    590080      activation_197[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_221 (BatchN (None, 4, 4, 256)    1024        conv2d_221[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_198 (Activation)     (None, 4, 4, 256)    0           batch_normalization_221[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_222 (Conv2D)             (None, 4, 4, 1024)   263168      activation_198[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_223 (Conv2D)             (None, 4, 4, 1024)   525312      activation_196[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_222 (BatchN (None, 4, 4, 1024)   4096        conv2d_222[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_223 (BatchN (None, 4, 4, 1024)   4096        conv2d_223[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_54 (Add)                    (None, 4, 4, 1024)   0           batch_normalization_222[0][0]    \n",
      "                                                                 batch_normalization_223[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_199 (Activation)     (None, 4, 4, 1024)   0           add_54[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_224 (Conv2D)             (None, 4, 4, 256)    262400      activation_199[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_224 (BatchN (None, 4, 4, 256)    1024        conv2d_224[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_200 (Activation)     (None, 4, 4, 256)    0           batch_normalization_224[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_225 (Conv2D)             (None, 4, 4, 256)    590080      activation_200[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_225 (BatchN (None, 4, 4, 256)    1024        conv2d_225[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_201 (Activation)     (None, 4, 4, 256)    0           batch_normalization_225[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_226 (Conv2D)             (None, 4, 4, 1024)   263168      activation_201[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_226 (BatchN (None, 4, 4, 1024)   4096        conv2d_226[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_55 (Add)                    (None, 4, 4, 1024)   0           batch_normalization_226[0][0]    \n",
      "                                                                 activation_199[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_202 (Activation)     (None, 4, 4, 1024)   0           add_55[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_227 (Conv2D)             (None, 4, 4, 256)    262400      activation_202[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_227 (BatchN (None, 4, 4, 256)    1024        conv2d_227[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_203 (Activation)     (None, 4, 4, 256)    0           batch_normalization_227[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_228 (Conv2D)             (None, 4, 4, 256)    590080      activation_203[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_228 (BatchN (None, 4, 4, 256)    1024        conv2d_228[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_204 (Activation)     (None, 4, 4, 256)    0           batch_normalization_228[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_229 (Conv2D)             (None, 4, 4, 1024)   263168      activation_204[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_229 (BatchN (None, 4, 4, 1024)   4096        conv2d_229[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_56 (Add)                    (None, 4, 4, 1024)   0           batch_normalization_229[0][0]    \n",
      "                                                                 activation_202[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_205 (Activation)     (None, 4, 4, 1024)   0           add_56[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_230 (Conv2D)             (None, 4, 4, 256)    262400      activation_205[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_230 (BatchN (None, 4, 4, 256)    1024        conv2d_230[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_206 (Activation)     (None, 4, 4, 256)    0           batch_normalization_230[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_231 (Conv2D)             (None, 4, 4, 256)    590080      activation_206[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_231 (BatchN (None, 4, 4, 256)    1024        conv2d_231[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_207 (Activation)     (None, 4, 4, 256)    0           batch_normalization_231[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_232 (Conv2D)             (None, 4, 4, 1024)   263168      activation_207[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_232 (BatchN (None, 4, 4, 1024)   4096        conv2d_232[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_57 (Add)                    (None, 4, 4, 1024)   0           batch_normalization_232[0][0]    \n",
      "                                                                 activation_205[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_208 (Activation)     (None, 4, 4, 1024)   0           add_57[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_233 (Conv2D)             (None, 4, 4, 256)    262400      activation_208[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_233 (BatchN (None, 4, 4, 256)    1024        conv2d_233[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_209 (Activation)     (None, 4, 4, 256)    0           batch_normalization_233[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_234 (Conv2D)             (None, 4, 4, 256)    590080      activation_209[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_234 (BatchN (None, 4, 4, 256)    1024        conv2d_234[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_210 (Activation)     (None, 4, 4, 256)    0           batch_normalization_234[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_235 (Conv2D)             (None, 4, 4, 1024)   263168      activation_210[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_235 (BatchN (None, 4, 4, 1024)   4096        conv2d_235[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_58 (Add)                    (None, 4, 4, 1024)   0           batch_normalization_235[0][0]    \n",
      "                                                                 activation_208[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_211 (Activation)     (None, 4, 4, 1024)   0           add_58[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_236 (Conv2D)             (None, 4, 4, 256)    262400      activation_211[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_236 (BatchN (None, 4, 4, 256)    1024        conv2d_236[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_212 (Activation)     (None, 4, 4, 256)    0           batch_normalization_236[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_237 (Conv2D)             (None, 4, 4, 256)    590080      activation_212[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_237 (BatchN (None, 4, 4, 256)    1024        conv2d_237[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_213 (Activation)     (None, 4, 4, 256)    0           batch_normalization_237[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_238 (Conv2D)             (None, 4, 4, 1024)   263168      activation_213[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_238 (BatchN (None, 4, 4, 1024)   4096        conv2d_238[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_59 (Add)                    (None, 4, 4, 1024)   0           batch_normalization_238[0][0]    \n",
      "                                                                 activation_211[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_214 (Activation)     (None, 4, 4, 1024)   0           add_59[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_239 (Conv2D)             (None, 2, 2, 512)    524800      activation_214[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_239 (BatchN (None, 2, 2, 512)    2048        conv2d_239[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_215 (Activation)     (None, 2, 2, 512)    0           batch_normalization_239[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_240 (Conv2D)             (None, 2, 2, 512)    2359808     activation_215[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_240 (BatchN (None, 2, 2, 512)    2048        conv2d_240[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_216 (Activation)     (None, 2, 2, 512)    0           batch_normalization_240[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_241 (Conv2D)             (None, 2, 2, 2048)   1050624     activation_216[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_242 (Conv2D)             (None, 2, 2, 2048)   2099200     activation_214[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_241 (BatchN (None, 2, 2, 2048)   8192        conv2d_241[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_242 (BatchN (None, 2, 2, 2048)   8192        conv2d_242[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_60 (Add)                    (None, 2, 2, 2048)   0           batch_normalization_241[0][0]    \n",
      "                                                                 batch_normalization_242[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_217 (Activation)     (None, 2, 2, 2048)   0           add_60[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_243 (Conv2D)             (None, 2, 2, 512)    1049088     activation_217[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_243 (BatchN (None, 2, 2, 512)    2048        conv2d_243[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_218 (Activation)     (None, 2, 2, 512)    0           batch_normalization_243[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_244 (Conv2D)             (None, 2, 2, 512)    2359808     activation_218[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_244 (BatchN (None, 2, 2, 512)    2048        conv2d_244[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_219 (Activation)     (None, 2, 2, 512)    0           batch_normalization_244[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_245 (Conv2D)             (None, 2, 2, 2048)   1050624     activation_219[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_245 (BatchN (None, 2, 2, 2048)   8192        conv2d_245[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_61 (Add)                    (None, 2, 2, 2048)   0           batch_normalization_245[0][0]    \n",
      "                                                                 activation_217[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_220 (Activation)     (None, 2, 2, 2048)   0           add_61[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_246 (Conv2D)             (None, 2, 2, 512)    1049088     activation_220[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_246 (BatchN (None, 2, 2, 512)    2048        conv2d_246[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_221 (Activation)     (None, 2, 2, 512)    0           batch_normalization_246[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_247 (Conv2D)             (None, 2, 2, 512)    2359808     activation_221[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_247 (BatchN (None, 2, 2, 512)    2048        conv2d_247[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_222 (Activation)     (None, 2, 2, 512)    0           batch_normalization_247[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_248 (Conv2D)             (None, 2, 2, 2048)   1050624     activation_222[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_248 (BatchN (None, 2, 2, 2048)   8192        conv2d_248[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_62 (Add)                    (None, 2, 2, 2048)   0           batch_normalization_248[0][0]    \n",
      "                                                                 activation_220[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_223 (Activation)     (None, 2, 2, 2048)   0           add_62[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, 1, 1, 2048)   0           activation_223[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 2048)         0           average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 6)            12294       flatten_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 23,600,006\n",
      "Trainable params: 23,546,886\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = ResNet50(input_shape = (64, 64, 3), classes = 6)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-866b891ec47ccb7b",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mAll tests passed!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from outputs import ResNet50_summary\n",
    "\n",
    "model = ResNet50(input_shape = (64, 64, 3), classes = 6)\n",
    "\n",
    "comparator(summary(model), ResNet50_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the Keras Tutorial Notebook, prior to training a model, you need to configure the learning process by compiling the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is now ready to be trained. The only thing you need now is a dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples = 1080\n",
      "number of test examples = 120\n",
      "X_train shape: (1080, 64, 64, 3)\n",
      "Y_train shape: (1080, 6)\n",
      "X_test shape: (120, 64, 64, 3)\n",
      "Y_test shape: (120, 6)\n"
     ]
    }
   ],
   "source": [
    "X_train_orig, Y_train_orig, X_test_orig, Y_test_orig, classes = load_dataset()\n",
    "\n",
    "# Normalize image vectors\n",
    "X_train = X_train_orig / 255.\n",
    "X_test = X_test_orig / 255.\n",
    "\n",
    "# Convert training and test labels to one hot matrices\n",
    "Y_train = convert_to_one_hot(Y_train_orig, 6).T\n",
    "Y_test = convert_to_one_hot(Y_test_orig, 6).T\n",
    "\n",
    "print (\"number of training examples = \" + str(X_train.shape[0]))\n",
    "print (\"number of test examples = \" + str(X_test.shape[0]))\n",
    "print (\"X_train shape: \" + str(X_train.shape))\n",
    "print (\"Y_train shape: \" + str(Y_train.shape))\n",
    "print (\"X_test shape: \" + str(X_test.shape))\n",
    "print (\"Y_test shape: \" + str(Y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to train your model on 10 epochs with a batch size of 32. On a GPU, it should take less than 2 minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "34/34 [==============================] - 1s 26ms/step - loss: 1.9666 - accuracy: 0.4574\n",
      "Epoch 2/10\n",
      "34/34 [==============================] - 1s 23ms/step - loss: 0.9559 - accuracy: 0.7009\n",
      "Epoch 3/10\n",
      "34/34 [==============================] - 1s 23ms/step - loss: 0.4299 - accuracy: 0.8435\n",
      "Epoch 4/10\n",
      "34/34 [==============================] - 1s 23ms/step - loss: 0.2447 - accuracy: 0.9111\n",
      "Epoch 5/10\n",
      "34/34 [==============================] - 1s 23ms/step - loss: 0.2976 - accuracy: 0.9120\n",
      "Epoch 6/10\n",
      "34/34 [==============================] - 1s 23ms/step - loss: 0.1704 - accuracy: 0.9370\n",
      "Epoch 7/10\n",
      "34/34 [==============================] - 1s 23ms/step - loss: 0.4447 - accuracy: 0.8889\n",
      "Epoch 8/10\n",
      "34/34 [==============================] - 1s 23ms/step - loss: 0.9113 - accuracy: 0.7204\n",
      "Epoch 9/10\n",
      "34/34 [==============================] - 1s 23ms/step - loss: 0.3557 - accuracy: 0.8833\n",
      "Epoch 10/10\n",
      "34/34 [==============================] - 1s 23ms/step - loss: 0.3125 - accuracy: 0.9028\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f2af43e3b70>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, epochs = 10, batch_size = 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "```\n",
    "Epoch 1/10\n",
    "34/34 [==============================] - 1s 34ms/step - loss: 1.9241 - accuracy: 0.4620\n",
    "Epoch 2/10\n",
    "34/34 [==============================] - 2s 57ms/step - loss: 0.6403 - accuracy: 0.7898\n",
    "Epoch 3/10\n",
    "34/34 [==============================] - 1s 24ms/step - loss: 0.3744 - accuracy: 0.8731\n",
    "Epoch 4/10\n",
    "34/34 [==============================] - 2s 44ms/step - loss: 0.2220 - accuracy: 0.9231\n",
    "Epoch 5/10\n",
    "34/34 [==============================] - 2s 57ms/step - loss: 0.1333 - accuracy: 0.9583\n",
    "Epoch 6/10\n",
    "34/34 [==============================] - 2s 52ms/step - loss: 0.2243 - accuracy: 0.9444\n",
    "Epoch 7/10\n",
    "34/34 [==============================] - 2s 48ms/step - loss: 0.2913 - accuracy: 0.9102\n",
    "Epoch 8/10\n",
    "34/34 [==============================] - 1s 30ms/step - loss: 0.2269 - accuracy: 0.9306\n",
    "Epoch 9/10\n",
    "34/34 [==============================] - 2s 46ms/step - loss: 0.1113 - accuracy: 0.9630\n",
    "Epoch 10/10\n",
    "34/34 [==============================] - 2s 57ms/step - loss: 0.0709 - accuracy: 0.9778\n",
    "```\n",
    "\n",
    "The exact values could not match, but don't worry about that. The important thing that you must see is that the loss value decreases, and the accuracy increases for the firsts 5 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how this model (trained on only two epochs) performs on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 7ms/step - loss: 0.3864 - accuracy: 0.9000\n",
      "Loss = 0.38636988401412964\n",
      "Test Accuracy = 0.8999999761581421\n"
     ]
    }
   ],
   "source": [
    "preds = model.evaluate(X_test, Y_test)\n",
    "print (\"Loss = \" + str(preds[0]))\n",
    "print (\"Test Accuracy = \" + str(preds[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <b>Test Accuracy</b>\n",
    "        </td>\n",
    "        <td>\n",
    "           >0.80\n",
    "        </td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of this assignment, you've been asked to train the model for just two epochs. You can see that it performs pretty poorly, but that's ok! The online grader will only run your code for a small number of epochs as well. Please go ahead and submit your assignment as is. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you have finished this official (graded) part of this assignment, you can also optionally train the ResNet for more iterations, if you want. It tends to get much better performance when trained for ~20 epochs, but this does take more than an hour when training on a CPU. \n",
    "\n",
    "Using a GPU, this ResNet50 model's weights were trained on the SIGNS dataset. You can load and run the trained model on the test set in the cells below. It may take ≈1min to load the model. Have fun! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_trained_model = tf.keras.models.load_model('resnet50.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 7ms/step - loss: 0.1596 - accuracy: 0.9500\n",
      "Loss = 0.15958674252033234\n",
      "Test Accuracy = 0.949999988079071\n"
     ]
    }
   ],
   "source": [
    "preds = pre_trained_model.evaluate(X_test, Y_test)\n",
    "print (\"Loss = \" + str(preds[0]))\n",
    "print (\"Test Accuracy = \" + str(preds[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Congratulations** on finishing this assignment! You've now implemented a state-of-the-art image classification system! Woo hoo! \n",
    "\n",
    "ResNet50 is a powerful model for image classification when it's trained for an adequate number of iterations. Hopefully, from this point, you can use what you've learned and apply it to your own classification problem to perform state-of-the-art accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'blue'>\n",
    "\n",
    "**What you should remember**:\n",
    "\n",
    "- Very deep \"plain\" networks don't work in practice because vanishing gradients make them hard to train.  \n",
    "- Skip connections help address the Vanishing Gradient problem. They also make it easy for a ResNet block to learn an identity function. \n",
    "- There are two main types of blocks: The **identity block** and the **convolutional block**. \n",
    "- Very deep Residual Networks are built by stacking these blocks together."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
